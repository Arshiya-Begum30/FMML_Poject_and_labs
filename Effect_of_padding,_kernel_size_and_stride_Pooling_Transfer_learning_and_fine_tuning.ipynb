{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN7xW+88bTocdYxr8dTvQch",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Arshiya-Begum30/FMML_Poject_and_labs/blob/main/Effect_of_padding%2C_kernel_size_and_stride_Pooling_Transfer_learning_and_fine_tuning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **1. Effect of padding, kernel size and stride**"
      ],
      "metadata": {
        "id": "d_vYHsIbZstS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Questions:\n",
        "\n",
        "1) Does increasing stride increase output image size?\n",
        "\n",
        "**Answer:**\n",
        "\n",
        " No, increasing the stride typically reduces the output image size.\n",
        "\n",
        "In convolutional neural networks (CNNs), the stride refers to the number of pixels the filter moves between each application to the input image. When we increase the stride, the filter moves over the input image in larger steps, which leads to a reduction in the size of the output feature maps.\n",
        "\n",
        "Conversely, decreasing the stride would result in smaller steps, potentially leading to larger output feature maps depending on the padding and size of the input."
      ],
      "metadata": {
        "id": "YJrgEFy-Z8Ur"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2) Does increasing padding increase output image size?\n",
        "\n",
        "**Answer:**\n",
        "\n",
        " Yes, increasing padding can increase the output image size, particularly when using convolutional layers in neural networks.\n",
        "\n",
        "Padding involves adding extra pixels (usually with zero values) around the input image before applying convolution. This extra border of pixels helps to retain information at the edges of the input, preventing a reduction in spatial dimensions. When we increase the padding, we add more extra pixels around the input, which can result in larger output feature maps."
      ],
      "metadata": {
        "id": "sk5qkvA0a0pG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **2. Pooling**"
      ],
      "metadata": {
        "id": "HthPS6dfbsCq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Questions:\n",
        "\n",
        "1) Can you think of any other pooling other than max and avg?\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "Yes, besides max pooling and average pooling, there are other types of pooling operations used in convolutional neural networks (CNNs). Some examples include:\n",
        "\n",
        "**1.** **Global Average Pooling (GAP):** This pooling operation computes the average value of each feature map across the entire spatial dimensions. It reduces each feature map to a single value, which is then used as a feature for classification.\n",
        "\n",
        "**2. Global Max Pooling (GMP):** Similar to global average pooling, global max pooling computes the maximum value of each feature map across all spatial dimensions.\n",
        "\n",
        "**3. Fractional Pooling:** This type of pooling allows for non-integer pool sizes and strides. It can be useful when we want to downsample an input by a fractional factor.\n",
        "\n",
        "**4. Min Pooling:** Similar to Max Pooling, but instead of taking the maximum value, it takes the minimum value from each local region.\n",
        "\n",
        "**5. Stochastic Pooling:** In this method, instead of taking the maximum or average value from a region, it randomly selects one value based on a probability distribution."
      ],
      "metadata": {
        "id": "b8mIZVwJbxXt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **3. Fine-tuning and transfer learning**"
      ],
      "metadata": {
        "id": "4kKVgAMQdemc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercises:"
      ],
      "metadata": {
        "id": "lfupniRKdu4E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q1: Why do you think the network did not achieve good test accuracy in the feature extraction approach?\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "There could be several reasons why a neural network did not achieve good test accuracy in a feature extraction approach. Here are some potential factors to consider:\n",
        "\n",
        "**1. Learning Rate:** In the feature extraction approach, we used a relatively high learning rate (0.01) for the optimizer that updates only the parameters of the newly added fully connected layer. This might be too high for the specific task or cause instability in fine-tuning. Experimenting with different learning rates might help.\n",
        "\n",
        "**2. Limited Capacity:** The feature extraction approach might not have sufficient model capacity to capture the complexity of the underlying patterns in the data. Increasing the depth or width of the network, or trying more advanced architectures, could be beneficial.\n",
        "\n",
        "**3. Number of Epochs:** The feature extraction training was conducted for only 5 epochs. For transfer learning, especially when fine-tuning a pre-trained model, it's often necessary to train for more epochs. We might want to consider increasing the number of epochs and monitoring the validation performance.\n",
        "\n",
        "**4. Fine-tuning More Layers:** In some cases, especially when the target task is significantly different from the source task, fine-tuning more layers of the pre-trained model may be beneficial. We could try unfreezing and fine-tuning more layers to capture task-specific features."
      ],
      "metadata": {
        "id": "7K0TJpwIdxp1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q2: Can you think of a scenario where the feature extraction approach would be preferred compared to fine tuning approach?\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "This can be suitable in scenarios where:\n",
        "\n",
        "**1. Limited Data:** If we have a relatively small dataset for your specific task (classification of German traffic signs), fine-tuning the entire pre-trained ResNet18 model might lead to overfitting due to the risk of learning task-specific features from a limited amount of data. Feature extraction allows us to leverage the pre-trained features and introduce minimal task-specific learning.\n",
        "\n",
        "**2. Task Similarity:** The pre-trained ResNet18 model is likely to have learned useful hierarchical features that are transferrable to tasks with similar visual characteristics, such as image classification. If the pre-trained features are relevant to the task, freezing most of the pre-trained model and only training a new classifier might be sufficient.\n",
        "\n",
        "**3. Resource Constraints:** Fine-tuning a deep neural network with a large number of parameters requires more computational resources compared to feature extraction. If we have limited resources, feature extraction provides a computationally less expensive option.\n",
        "\n",
        "**4. Preventing Overfitting:** Freezing the majority of the pre-trained model helps prevent overfitting, especially when the task-specific dataset is small. The pre-trained model acts as a feature extractor, and the new classifier is trained to make predictions based on these extracted features."
      ],
      "metadata": {
        "id": "TpGc_0Mdhudj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q3: Replace the ResNet18 architecture with some other pretrained model in pytorch and try to find the optimal parameters. Report the architecture and the final model performance.\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "We can replace the ResNet18 architecture with a different pre-trained model. Let's replace it with the VGG16 architecture."
      ],
      "metadata": {
        "id": "KLZ5ozEKiQpA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k61J8S_yZWk1"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torch.utils.data import DataLoader, SubsetRandomSampler\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import time\n",
        "\n",
        "# Device configuration (whether to run on GPU or CPU)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)   # Set seeds for reproducibility\n",
        "seed = 0\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed(seed)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "\n",
        "# Download and unzip the dataset\n",
        "!gdown --id 1V7dt70fz_AKRJlttyjnrtFpuJDLXr15x\n",
        "!unzip -q german_traffic_signs_dataset.zip\n",
        "\n",
        "# Transformation for data augmentation\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.GaussianBlur(3),\n",
        "    transforms.RandomAffine(0, translate=(0.3, 0.3), shear=5),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "# Load the dataset\n",
        "trainset = ImageFolder('german_traffic_signs_dataset/Train', transform=transform)\n",
        "testset = ImageFolder('german_traffic_signs_dataset/Test', transform=transform)\n",
        "\n",
        "# Shuffle and split train set into 80% training and 20% validation set\n",
        "val_split = 0.2\n",
        "indices = np.arange(len(trainset))\n",
        "np.random.shuffle(indices)\n",
        "partition = int((1 - val_split) * len(trainset))\n",
        "\n",
        "# SubsetRandomSampler will only sample examples from the given subset of data\n",
        "train_loader = DataLoader(trainset, shuffle=False, sampler=SubsetRandomSampler(indices[:partition]), batch_size=64, num_workers=2)\n",
        "val_loader = DataLoader(trainset, shuffle=False, sampler=SubsetRandomSampler(indices[partition:]), batch_size=64, num_workers=2)\n",
        "\n",
        "dataloaders = {'train': train_loader, 'val': val_loader}\n",
        "dataset_sizes = {'train': partition, 'val': len(train_loader.dataset) - partition}\n",
        "\n",
        "test_loader = DataLoader(testset, shuffle=False, batch_size=64, num_workers=2)\n",
        "\n",
        "# Print dataset information\n",
        "print('Number of training images: ', dataset_sizes['train'])\n",
        "print('Number of validation images: ', dataset_sizes['val'])\n",
        "print('Number of test images: ', len(test_loader.dataset))\n",
        "print('Number of classes: ', len(trainset.classes))\n",
        "\n",
        "# Helper function to show an image\n",
        "def plot_image(img):\n",
        "    img = img / 2 + 0.5  # unnormalize the image\n",
        "    npimg = img.numpy()  # torch to numpy\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))  # as torch image is (C, H, W)\n",
        "    plt.show()\n",
        "\n",
        "# Get some random training images from dataloader\n",
        "dataiter = iter(train_loader)\n",
        "images, labels = next(dataiter)\n",
        "\n",
        "# Plot images\n",
        "plot_image(torchvision.utils.make_grid(images[:20], nrow=5))\n",
        "\n",
        "# Define a custom classifier for VGG16\n",
        "class CustomVGG16Classifier(nn.Module):\n",
        "    def __init__(self, num_classes=43):\n",
        "        super(CustomVGG16Classifier, self).__init__()\n",
        "        self.features = torchvision.models.vgg16(pretrained=True).features\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((7, 7))\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(512 * 7 * 7, 4096),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(),\n",
        "            nn.Linear(4096, 4096),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(),\n",
        "            nn.Linear(4096, num_classes),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = self.avgpool(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "\n",
        "# Initialize the model\n",
        "model = CustomVGG16Classifier(num_classes=43).to(device)\n",
        "\n",
        "# Define loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 10\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    for inputs, labels in train_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    # Print training statistics\n",
        "    print(f'Epoch {epoch+1}/{num_epochs}, Loss: {loss.item()}')\n",
        "\n",
        "# Evaluation on the test set\n",
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in test_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        outputs = model(inputs)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "accuracy = correct / total\n",
        "print(f'Test Accuracy: {accuracy}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Some of the parameters that can be considered for optimization include learning rate, number of epochs, and model architecture."
      ],
      "metadata": {
        "id": "vkUo4Ft6jcpl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q4: Which other data augmentations can we used to augment the data?\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "Other common data augmentations that can be used to further augment the data include:\n",
        "\n",
        "**1. Random Rotation:** Randomly rotating the image by a certain angle.\n",
        "\n",
        "**2. Color Jitter:** Randomly changing the brightness, contrast, saturation, and hue of the image.\n",
        "\n",
        "**3. Random Horizontal Flip:** Flipping the image horizontally with a certain probability.\n",
        "\n",
        "**4. Random Vertical Flip:** Flipping the image vertically with a certain probability.\n",
        "\n",
        "**5. Random Crop:** Randomly cropping a portion of the image."
      ],
      "metadata": {
        "id": "CWDtIvN5jrGn"
      }
    }
  ]
}